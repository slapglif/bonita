Skip to content
LangMem
Core Concepts


 
logoLangMem
 GitHub
Introduction
Quickstart üèéÔ∏è
Quickstart üèéÔ∏è
üöÄ Hot Path Quickstart
üß† Background Quickstart
Concepts üí°
Concepts üí°
Core Concepts
Table of contents
Types of Memory
Semantic Memory: Facts and Knowledge
Collection
Profiles
Episodic Memory: Past Experiences
Procedural Memory: System Instructions
Writing memories
Conscious Formation
Subconcious Formation
Integration Patterns
1. Core API
2. Stateful Integration
Storage System
Memory Namespaces
Flexible Retrieval
How-to Guides üî®
How-to Guides üî®
Defer Background Memory Processing
Manage a Semantic Memory Collection
Manage User Profiles
Manage Episodic Memories
Use Memory Tools
Optimize Single Prompts
Optimize Multiple Prompts
Configure Dynamic Namespaces
Use Tools in Custom Agents
Use Tools in CrewAI
Summarize Message History
API Reference üìì
API Reference üìì
Extractive Memory
Tools
Prompt Optimization
Utils
Short Term Memory
Long-term Memory in LLM Applications¬∂
Long-term memory allows agents to remember important information across conversations. LangMem provides ways to extract meaningful details from chats, store them, and use them to improve future interactions. At its core, each memory operation in LangMem follows the same pattern:

Accept conversation(s) and current memory state
Prompt an LLM to determine how to expand or consolidate the memory state
Respond with the updated memory state
The best memory systems are often application-specific. In designing yours, the following questions can serve as a useful guide:

What type of content should your agent learn: facts/knowledge? summary of past events? Rules and style?
When should the memories be formed (and who should form the memories)
Where should memories be stored? (in the prompt? Semantic store?). This largely determines how they will be recalled.
Types of Memory¬∂
Memory in LLM applications can reflect some of the structure of human memory, with each type serving a distinct purpose in building adaptive, context-aware systems:

Memory Type	Purpose	Agent Example	Human Example	Typical Storage Pattern
Semantic	Facts & Knowledge	User preferences; knowledge triplets	Knowing Python is a programming language	Profile or Collection
Episodic	Past Experiences	Few-shot examples; Summaries of past conversations	Remembering your first day at work	Collection
Procedural	System Behavior	Core personality and response patterns	Knowing how to ride a bicycle	Prompt rules or Collection
Semantic Memory: Facts and Knowledge¬∂
Semantic memory stores the essential facts and other information that ground an agent's responses. Two common representations of semantic memory are collections (to record an unbounded amount of knowledge to be searched at runtime) and profiles (to record task-specific information that follows a strict schema that is easily looked up by user or agent).

Collection¬∂
Collections are what most people think of when they imagine agent long-term memory. In this type, memories are stored as individual documents or records. For each new conversation, the memory system can decide to insert new memories to the store.

Using a collection-type memory adds some complexity to the process of updating your memory state. The system must reconcile new information with previous beliefes, either deleting/invalidating or updating/consolidating existing memories. If the system over-extracts, this could lead to reduced precision of memories when your agent needs to search the store. If it under-extracts, this could lead to low recall. LangMem uses a memory enrichment process that strives to balance memory creation and consolidation, while letting you, the developer, customize the instructions to further shift the strength of each.

Finally, memory relevance is more than just semantic similarity. Recall should combine similarity with "importance" of the memory, as well as the memory's "strength", which is a function of how recently/frequently it was used.

Collection update process

Extracting semantic memories as collections
Profiles¬∂
Profiles on the other hand are well-scoped for a particular task. Profiles are a single document that represents the current state, like a user's main goals with using an app, their preferred name and response stele, etc. When new information arrives, it updates the existing document rather than creating a new one. This approach is ideal when you only care about the latest state and want to avoid remembering extraneous information.

Profile update process

Managing user preferences with profiles
Choose between profiles and collections based on how you'll use the data: profiles excel when you need quick access to current state and when you have data requirements about what type of information you can store. They are also easy to present to a user for manual editing. Collections are useful when you want to track knowledge across many interactions without loss of information, and when you want to recall certain information contextually rather than every time.

Episodic Memory: Past Experiences¬∂
Episodic memory preserves successful interactions as learning examples that guide future behavior. Unlike semantic memory which stores facts, episodic memory captures the full context of an interaction‚Äîthe situation, the thought process that led to success, and why that approach worked. These memories help the agent learn from experience, adapting its responses based on what has worked before.

Defining and extracting episodes
Procedural Memory: System Instructions¬∂
Procedural memory encodes how an agent should behave and respond. It starts with system prompts that define core behavior, then evolves through feedback and experience. As the agent interacts with users, it refines these instructions, learning which approaches work best for different situations.

Instructions update process

Optimizing prompts based on feedback
Writing memories¬∂
Memories can form in two ways, each suited for different needs. Active formation happens during conversations, enabling immediate updates when critical context emerges. Background formation occurs between interactions, allowing deeper pattern analysis without impacting response time. This dual approach lets you balance responsiveness with thorough learning.

Formation Type	Latency Impact	Update Speed	Processing Load	Use Case
Active	Higher	Immediate	During Response	Critical Context Updates
Background	None	Delayed	Between/After Calls	Pattern Analysis, Summaries
Hot path vs background memory processing

Conscious Formation¬∂
You may want your agent to save memories "in the hot path." This active memory formation happens during the conversation, enabling immediate updates when critical context emerges. This approach is easy to implement and lets the agent itself choose how to store and update its memory. However, it adds perceptible latency to user interactions, and it adds one more obstacle to the agent's ability to satisfy the user's needs.

Check out the "hot path" quickstart for an example of how to use this technique.

Subconcious Formation¬∂
"Subconcious" memory formation refers to the technique of prompting an LLM to reflect on a conversation after it occurs (or after it has been inactive for some period), finding patterns and extracting insights without slowing down the immediate interaction or adding complexity to the agent's tool choice decisions. This approach is perfect for ensuring higher recall of exracted information.

Check out the "background" quickstart for an example of how to use this technique.

Integration Patterns¬∂
LangMem's memory utilities are organized in two layers of integration patterns:

1. Core API¬∂
At its heart, LangMem provides functions that transform memory state without side effects. These primitives are the building blocks for memory operations:

Memory Managers: Extract new memories, update or remove outdated memories, and consolidate and generalize from existing memories based on new conversation information
Prompt Optimizers: Update prompt rules and core behavior based on conversation information (with optional feedback)
These core functions do not depend on any particular database or storage system. You can use them in any application.

2. Stateful Integration¬∂
The next layer up depends on LangGraph's long-term memory store. These components use the core API above to transform memories that existing in the store and upsert/delete them as needed when new conversation information comesin:

Store Managers: Automatically persist extracted memories
Memory Management Tools: Give agents direct access to memory operations
Use these if you're using LangGraph Platform or LangGraph OSS, since it's an easy way to add memory capabilities to your agents.

Storage System¬∂
Storage is optional
When using LangMem's stateful operators or platform services, the storage system is built on LangGraph's storage primitives, providing a flexible and powerful way to organize and access memories. The storage system is designed around two concepts:

Memory Namespaces¬∂
Memories are organized into namespaces that allows for natural segmentation of data:

Multi-Level Namespaces: Group memories by organization, user, application, or any other hierarchical structure
Contextual Keys: Identify memories uniquely within their namespace
Structured Content: Store rich, structured data with metadata for better organization
Organizing memories hierarchically
Namespaces can include template variables (such as "{user_id}") to be populated at runtime from configurable fields in the RunnableConfig. See how to dynamically configure namespaces for an example, or the NamespaceTemplate reference docs for more details.

Flexible Retrieval¬∂
If you use one of the managed APIs, LangMem will integrate directly with LangGraph's BaseStore interface for memory storage and retrieval. The storage system supports multiple ways to retrieve memories:

Direct Access: Get a specific memory by key
Semantic Search: Find memories by semantic similarity
Metadata Filtering: Filter memories by their attributes
For more details on storage capabilities, see the LangGraph Storage documentation.

Comments

 Back to top
Previous
üß† Background Quickstart
Next
Defer Background Memory Processing
Made with Material for MkDocs
